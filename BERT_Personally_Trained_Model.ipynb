{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff35c8ec-d6ab-420c-b35c-af1e74ba0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, Subset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from belief_maps import filenames, combined_belief_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272603d-2029-4f6e-bc29-7a3f934ef48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch transformers scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278b8ce6-9701-4a30-8856-3564e16e8377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# labels texts based on a belief map (0 for 'Negative', 1 for 'Positive'),\n",
    "# and returns two lists: texts containing the file content and \n",
    "# labels for classification. \n",
    "def read_text_files(directory, filenames, belief_map):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                texts.append(text)\n",
    "                labels.append(0 if belief_map[filename] == 'Negative' else 1)\n",
    "        else:\n",
    "            print(f\"File {filename} not found in directory {directory}.\")\n",
    "    return texts, labels\n",
    "\n",
    "# Take data from working directory in folder \"All_Texts\" and categorise by label\n",
    "directory = os.path.join(os.getcwd(), \"All_Texts\")\n",
    "\n",
    "texts, labels = read_text_files(directory, filenames, combined_belief_map)\n",
    "labels = [1 if combined_belief_map[filename] == 'Positive' else 0 for filename in filenames]\n",
    "\n",
    "# Split into training and test sets\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# The BERT tokenizer and model are initialized with\n",
    "# pre-trained weights from the 'bert-base-uncased' model, configured \n",
    "# for binary classification.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d831738-caf4-4266-99a9-74d6d9a4a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using BERT’s tokenizer, applying padding and truncation to ensure uniform input length, \n",
    "# and converting the outputs to PyTorch tensors with a maximum sequence length of 512 token\n",
    "\n",
    "train_inputs = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "val_inputs = tokenizer(val_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "test_inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=RandomSampler(train_dataset))\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8, sampler=SequentialSampler(val_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, sampler=SequentialSampler(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba35bc52-4009-41dd-a4dc-4f37bae67e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set up the AdamW optimizer with a learning rate of 2e-5 and epsilon of 1e-8 for numerical stability. \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check for GPU availability and move the model to the appropriate device (GPU if available, otherwise CPU)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# for efficient training.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up the AdamW optimizer with a learning rate of 2e-5 and epsilon of 1e-8 for numerical stability. \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Check for GPU availability and move the model to the appropriate device (GPU if available, otherwise CPU)\n",
    "# for efficient training.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model’s performance on a given data loader. The model is set to evaluation mode, \n",
    "# and predictions are made without gradient calculations (torch.no_grad()). For each batch, input \n",
    "# data and labels are moved to the appropriate device, and predictions are generated. Accuracy,\n",
    "# precision, recall, and F1 score are calculated based on the predictions and returned as evaluation metrics.\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc750b82-2da7-4078-99f5-9c210086e29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6, eps=1e-8, weight_decay=0.01)\n",
    "\n",
    "def run_training_and_evaluation():\n",
    "    \"\"\"Initialize the CrossEntropyLoss criterion and configure the AdamW optimizer with\n",
    "    a lower learning rate and weight decay for regularization. The run_training_and_evaluation \n",
    "    function performs 3-fold cross-validation on the training dataset, splitting it into different training\n",
    "    and validation subsets for each fold. Within each fold, a training loop runs for a specified number of \n",
    "    epochs, optimizing the model using backpropagation. After training, the model is evaluated on the \n",
    "    validation set using accuracy, precision, recall, and F1-score. Finally, the model’s performance is \n",
    "    evaluated on the test set, returning the test accuracy.\"\"\"\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(train_dataset)):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        train_subset = Subset(train_dataset, train_index)\n",
    "        val_subset = Subset(train_dataset, val_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=4, sampler=RandomSampler(train_subset))\n",
    "        validation_loader = DataLoader(val_subset, batch_size=4, sampler=SequentialSampler(val_subset))\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs.logits, batch[2])\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_train_loss}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        accuracy, precision, recall, f1 = evaluate(model, validation_loader)\n",
    "        print(f\"Fold {fold+1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Final evaluation on the test set\n",
    "    test_accuracy, test_precision, test_recall, test_f1 = evaluate(model, test_loader)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a56275c-0da4-4abf-95ac-dfe8699822d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.6681465581059456\n",
      "Epoch 2, Loss: 0.6434632353484631\n",
      "Epoch 3, Loss: 0.6305680051445961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.6000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.6327815987169743\n",
      "Epoch 2, Loss: 0.6591181084513664\n",
      "Epoch 3, Loss: 0.6957699060440063\n",
      "Fold 2 - Accuracy: 0.6000, Precision: 1.0000, Recall: 0.2500, F1 Score: 0.4000\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.6953234821557999\n",
      "Epoch 2, Loss: 0.6642161980271339\n",
      "Epoch 3, Loss: 0.6597122699022293\n",
      "Fold 3 - Accuracy: 0.7857, Precision: 1.0000, Recall: 0.4000, F1 Score: 0.5714\n",
      "Run 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.6113217882812023\n",
      "Epoch 2, Loss: 0.6669505089521408\n",
      "Epoch 3, Loss: 0.652454137802124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.6000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.6493916660547256\n",
      "Epoch 2, Loss: 0.635316863656044\n",
      "Epoch 3, Loss: 0.6614585742354393\n",
      "Fold 2 - Accuracy: 0.6667, Precision: 1.0000, Recall: 0.3750, F1 Score: 0.5455\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.6573622673749924\n",
      "Epoch 2, Loss: 0.6564330011606216\n",
      "Epoch 3, Loss: 0.6308974102139473\n",
      "Fold 3 - Accuracy: 0.9286, Precision: 1.0000, Recall: 0.8000, F1 Score: 0.8889\n",
      "Run 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.7111102491617203\n",
      "Epoch 2, Loss: 0.6819249987602234\n",
      "Epoch 3, Loss: 0.7054719999432564\n",
      "Fold 1 - Accuracy: 0.8000, Precision: 0.6667, Recall: 1.0000, F1 Score: 0.8000\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.7041812688112259\n",
      "Epoch 2, Loss: 0.6506929025053978\n",
      "Epoch 3, Loss: 0.6812635734677315\n",
      "Fold 2 - Accuracy: 0.6000, Precision: 0.7500, Recall: 0.3750, F1 Score: 0.5000\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.6702312082052231\n",
      "Epoch 2, Loss: 0.6655416637659073\n",
      "Epoch 3, Loss: 0.6413455978035927\n",
      "Fold 3 - Accuracy: 0.9286, Precision: 0.8333, Recall: 1.0000, F1 Score: 0.9091\n",
      "Run 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.6626507788896561\n",
      "Epoch 2, Loss: 0.6432201750576496\n",
      "Epoch 3, Loss: 0.6476310528814793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.6000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.6540769636631012\n",
      "Epoch 2, Loss: 0.6269956417381763\n",
      "Epoch 3, Loss: 0.6602695360779762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Accuracy: 0.4667, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.6821461990475655\n",
      "Epoch 2, Loss: 0.6584043502807617\n",
      "Epoch 3, Loss: 0.6617418080568314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Accuracy: 0.6429, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.6997877061367035\n",
      "Epoch 2, Loss: 0.6583363115787506\n",
      "Epoch 3, Loss: 0.6817423775792122\n",
      "Fold 1 - Accuracy: 0.2667, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.6844698712229729\n",
      "Epoch 2, Loss: 0.6694984063506126\n",
      "Epoch 3, Loss: 0.6595005020499229\n",
      "Fold 2 - Accuracy: 0.5333, Precision: 0.6667, Recall: 0.2500, F1 Score: 0.3636\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.7330388724803925\n",
      "Epoch 2, Loss: 0.675837229937315\n",
      "Epoch 3, Loss: 0.6818124651908875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Accuracy: 0.6429, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Run 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.6976071372628212\n",
      "Epoch 2, Loss: 0.6760995164513588\n",
      "Epoch 3, Loss: 0.6233232654631138\n",
      "Fold 1 - Accuracy: 0.6667, Precision: 1.0000, Recall: 0.1667, F1 Score: 0.2857\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.5968341380357742\n",
      "Epoch 2, Loss: 0.596122495830059\n",
      "Epoch 3, Loss: 0.6384907811880112\n",
      "Fold 2 - Accuracy: 0.6667, Precision: 1.0000, Recall: 0.3750, F1 Score: 0.5455\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.6381660848855972\n",
      "Epoch 2, Loss: 0.637650802731514\n",
      "Epoch 3, Loss: 0.6263613551855087\n",
      "Fold 3 - Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1 Score: 1.0000\n",
      "Run 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.7032582312822342\n",
      "Epoch 2, Loss: 0.6973480135202408\n",
      "Epoch 3, Loss: 0.6675153970718384\n",
      "Fold 1 - Accuracy: 0.6667, Precision: 1.0000, Recall: 0.1667, F1 Score: 0.2857\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.6692741960287094\n",
      "Epoch 2, Loss: 0.6600122973322868\n",
      "Epoch 3, Loss: 0.6536164283752441\n",
      "Fold 2 - Accuracy: 0.6667, Precision: 1.0000, Recall: 0.3750, F1 Score: 0.5455\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.6823406293988228\n",
      "Epoch 2, Loss: 0.6567135378718376\n",
      "Epoch 3, Loss: 0.6429727226495743\n",
      "Fold 3 - Accuracy: 0.9286, Precision: 1.0000, Recall: 0.8000, F1 Score: 0.8889\n",
      "Run 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.8053115531802177\n",
      "Epoch 2, Loss: 0.7891221195459366\n",
      "Epoch 3, Loss: 0.7416359111666679\n",
      "Fold 1 - Accuracy: 0.4000, Precision: 0.4000, Recall: 1.0000, F1 Score: 0.5714\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.7609340697526932\n",
      "Epoch 2, Loss: 0.7262724339962006\n",
      "Epoch 3, Loss: 0.7034813463687897\n",
      "Fold 2 - Accuracy: 0.6667, Precision: 0.6667, Recall: 0.7500, F1 Score: 0.7059\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.667810283601284\n",
      "Epoch 2, Loss: 0.7215845584869385\n",
      "Epoch 3, Loss: 0.7030531913042068\n",
      "Fold 3 - Accuracy: 0.8571, Precision: 1.0000, Recall: 0.6000, F1 Score: 0.7500\n",
      "Run 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.6870690137147903\n",
      "Epoch 2, Loss: 0.678669884800911\n",
      "Epoch 3, Loss: 0.6610021218657494\n",
      "Fold 1 - Accuracy: 0.4000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.6654654517769814\n",
      "Epoch 2, Loss: 0.6255526468157768\n",
      "Epoch 3, Loss: 0.6758321821689606\n",
      "Fold 2 - Accuracy: 0.6000, Precision: 0.7500, Recall: 0.3750, F1 Score: 0.5000\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.663232296705246\n",
      "Epoch 2, Loss: 0.6685125008225441\n",
      "Epoch 3, Loss: 0.6684006303548813\n",
      "Fold 3 - Accuracy: 0.7857, Precision: 1.0000, Recall: 0.4000, F1 Score: 0.5714\n",
      "Run 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aaron\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.8052205964922905\n",
      "Epoch 2, Loss: 0.7692593336105347\n",
      "Epoch 3, Loss: 0.7198833376169205\n",
      "Fold 1 - Accuracy: 0.7333, Precision: 0.7500, Recall: 0.5000, F1 Score: 0.6000\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.6868038401007652\n",
      "Epoch 2, Loss: 0.6505073495209217\n",
      "Epoch 3, Loss: 0.6204651184380054\n",
      "Fold 2 - Accuracy: 0.8000, Precision: 1.0000, Recall: 0.6250, F1 Score: 0.7692\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.6586970537900925\n",
      "Epoch 2, Loss: 0.6828380152583122\n",
      "Epoch 3, Loss: 0.6323940679430962\n",
      "Fold 3 - Accuracy: 0.7143, Precision: 0.6667, Recall: 0.4000, F1 Score: 0.5000\n",
      "Average Test Set Accuracy over 10 runs: 0.6400\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "total_accuracy = 0\n",
    "\n",
    "# Run the model 10 times to ensure reliability of results. For each run,\n",
    "# reinitialize the BERT model and optimizer to avoid any carryover effects \n",
    "# from previous runs. The accuracy from each run is accumulated, and the average\n",
    "# test set accuracy is calculated and printed at the end\n",
    "# for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    # Reinitialize the model and optimizer for each run\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-6, eps=1e-8, weight_decay=0.01)\n",
    "    \n",
    "    # Run the training and evaluation\n",
    "    accuracy = run_training_and_evaluation()\n",
    "    total_accuracy += accuracy\n",
    "\n",
    "average_accuracy = total_accuracy / num_runs\n",
    "print(f\"Average Test Set Accuracy over {num_runs} runs: {average_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
